Based on the tf-documentation, you can still pass the sparse tensor even if the sparse argument is set as False, as long as the input data is a sparse matrix.

sparse : A boolean specifying whether the placeholder to be created is sparse.
Only one of 'ragged' and 'sparse' can be True.
= tf.concat([np.c_[np.ones(x.shape[0])],x],1)
#the weights of the first layer are multiplied by the input of the first layer

#z1 = tf.sparse_tensor_dense_matmul(theta1, a1)


I have also fixed that in case you wanna stick with [4,16] for each input.
here is the code running
const trainingData = tf.tensor3d( 
         
# At least as many columns as k
ncols = tf.maximum(tf.math.reduce_max(col)
Input(batch_shape=(2,None,5), sparse=False)
x = tf.keras.layers.
Like,
input = tf.keras.
[4,2],name = "x")
#declaring a place holder for input x
y = tf.placeholder(tf.float32,shape =
Input(batch_shape=(2,None,5))
x = toDense()(input)
x = tf.keras.layers.
So the only way is to use a mask matrix.
* d, [[1, 0]])
col = r - tf.scan(tf.math.maximum, m)
# Get only up to k elements per row
m = col &lt; k
row_m = tf.boolean_mask(row, m)
col_m = tf.boolean_mask(col, m)
idx_m = tf.boolean_mask(sp2.values, m)
# Make result
scatter_idx = tf.stack([row_m, col_m], axis=-1)
scatter_shape =
It would be important to see your labels (output) data.
Layer):
    def call(self, input):
        if isinstance(input, tf.sparse.
If the SparseTensors have the same dense_shape you can create a unique SparseTensor instead of a list and pass it to from_tensor_slices.
For example the following code produce separate SparseTensors from a large SparseTensor s splitting them along the first dimension
s = tf.sparse.
z1 = tf.matmul(a1,tf.sparse_tensor_to_dense(theta1))
#the input of the second layer is the output of the first layer, passed through the 

a2 = tf.concat([np.c_[np.ones(x.shape[0])],tf.sigmoid(z1)],1)
#the input of the second layer is multiplied by the weights

z3 = tf.matmul(a2,theta2)
#the output is passed through the activation function to obtain the final probability

h3 = tf.sigmoid(z3)
cost_func = -tf.reduce_sum(y*tf.log(h3)+(1-y)*tf.log(1-h3),axis = 1)

#built in tensorflow optimizer that conducts gradient descent using specified 

optimiser = tf.train.
# Get in dense form
condensed = tf.sparse.to_dense(sp2)
# Top-k (indices do not correspond to initial sparse matrix)
values, indices = tf.math.top_k(condensed, k)
print(values.numpy())


To use from_tensor_slices in this way, you need a function to convert the list sparse_lists to a large SparseTensor s (reported below).
To recap, you can do
import tensorflow as tf

def sparse_list_to_sparse_tensor(sparse_lists):
    n = len(sparse_lists)
    shape = sparse_lists[0].dense_shape
    out_shape = (n, *shape)
    out_values = tf.concat([s.values for s in sparse_lists], axis=0)
    out_indices