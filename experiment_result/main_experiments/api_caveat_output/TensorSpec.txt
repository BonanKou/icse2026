Below code will show the difference between two.

Output
tf.int32

Output
tf.float32
now adding these two tensors will throw an error as data type for these two are different.
If you use the tf.data.
To get the model to allocate properly when using a model like this, you have to do 2 things:

Manually set the shape of the input tensor to be the shape of the input data, e.g. interpreter.resize_tensor_input(0, [1, input_shape[0], input_shape[1], 3], strict=True).
When using a CNN with unknown input dimensions (i.e. -1 in the shape_signature here, caused by setting -1 in the input layer) the unknown dimensions in the input tensor are set to 1.
Edit: Regarding setting the dtype of the input data, this is done in the casting to numpy.array after it is imported from an image format, before calling allocate_tensors().
It seems this is done automatically in regular TensorFlow, but the model must be prepared like this in the Lite version.
tensorflow_hub version:  0.12.0
tensorflow version:  2.2.0
I set up the following path on my Linux server:

(For various reasons, we have some needs for Tensorflow 1.x still, so I figured it might be a good idea to separate models based on if they are designed to work with tensorflow 1.x
So, when tf-models-official or slim is installed, the tf_slim package is installed as well (https://github.com/google-research/tf-slim/tree/master).
However, the 'tensor' module is only present starting from tensorflow==2.13.
You should define the output_shapes for the tf.data.
This is the user guide on how to use TensorRT in TF: https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html
This talk explains how TensorRT works in TF: https://developer.nvidia.com/gtc/2019/video/S9431
Note that TensorRT also supports INT8-quantization (during training or post-training).
See this example:

If you want anything else, look at this thread over at Data Science Stack Exchange
Introspecting the model.json we find a lot of "unused_control_flow_input_" which could relate to input tensor only used for training purposes.
We are having the same error.
However, we are only guessing and there is no documentation.
It also supports leveraging GPU and/or hardware accelerator if any of this is available to you.
https://www.tensorflow.org/lite
Your test set prints this: <TensorSliceDataset element_spec=(TensorSpec(shape=(32, 32, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(10,), dtype=tf.float32, name=None))
When I ran your code with that change, it worked.
Instead of .from_tensor_slices(), try .from_tensors().