{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import weka.core.jvm as jvm\n",
    "from weka.core.dataset import Instances, Instance\n",
    "from weka.filters import Filter\n",
    "from weka.core.converters import Loader, Saver\n",
    "import pandas as pd\n",
    "from weka.core.converters import Loader\n",
    "from weka.filters import Filter\n",
    "from weka.core.serialization import write, read\n",
    "from weka.core.jvm import start, stop\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(sentence):\n",
    "    soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    text = str(text)\n",
    "    return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "\n",
    "def token_similarity(string1, string2):\n",
    "    def tokenize(string):\n",
    "        return set(string.split())\n",
    "\n",
    "    tokens_a = tokenize(string1)\n",
    "    tokens_b = tokenize(string2)\n",
    "    intersection = len(tokens_a & tokens_b)\n",
    "    magnitude_a = math.sqrt(len(tokens_a))\n",
    "    magnitude_b = math.sqrt(len(tokens_b))\n",
    "    if magnitude_a == 0 or magnitude_b == 0:\n",
    "        return 0.0\n",
    "    similarity = intersection / (magnitude_a * magnitude_b)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def count_tokens(sentence):\n",
    "    sentence = remove_html(sentence)\n",
    "    return len(sentence.split())\n",
    "\n",
    "\n",
    "def is_codeblock(df):\n",
    "    inside_pre = False\n",
    "    pre_sentences = []\n",
    "    for sentence in df[\"sentence\"]:\n",
    "        if \"<pre>\" in sentence:\n",
    "            inside_pre = True\n",
    "        if inside_pre:\n",
    "            pre_sentences.append(1)\n",
    "        else:\n",
    "            pre_sentences.append(0)\n",
    "        if \"</pre>\" in sentence:\n",
    "            inside_pre = False\n",
    "    df[\"is_codeblock\"] = pre_sentences\n",
    "    return df\n",
    "\n",
    "\n",
    "def api_position(sentence, api_name):\n",
    "    position = sentence.find(api_name)\n",
    "    return position + 1 if position != -1 else 0\n",
    "\n",
    "\n",
    "def starts_lower_case(sentence):\n",
    "    sentence = remove_html(sentence)\n",
    "    return 1 if sentence and sentence[0].islower() else 0\n",
    "\n",
    "\n",
    "def code_token_count(df):\n",
    "    code_token = []\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row[\"sentence\"]\n",
    "        is_codeblock = row[\"is_codeblock\"]\n",
    "        cleaned_sentence = remove_html(sentence)\n",
    "        if is_codeblock:\n",
    "            total_length = len(sentence)\n",
    "            pre_start = sentence.find(\"<pre>\")\n",
    "            pre_end = sentence.find(\"</pre>\")\n",
    "            if pre_start == -1 and pre_end == -1:\n",
    "                code_token.append(total_length)\n",
    "            elif pre_start == -1:\n",
    "                code_token.append(pre_end)\n",
    "            elif pre_end == -1:\n",
    "                code_token.append(total_length - pre_start)\n",
    "            else:\n",
    "                soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "                code_elements = soup.find_all([\"pre\", \"code\"])\n",
    "                token_num = sum(len(code.get_text()) for code in code_elements)\n",
    "                code_token.append(token_num)\n",
    "        else:\n",
    "            soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "            code_elements = soup.find_all([\"pre\", \"code\"])\n",
    "            token_num = sum(len(code.get_text()) for code in code_elements)\n",
    "            code_token.append(token_num)\n",
    "    df[\"num_code_characters\"] = code_token\n",
    "\n",
    "\n",
    "def contains_html_tags(sentence):\n",
    "    soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "    tags = [\n",
    "        \"code\",\n",
    "        \"pre\",\n",
    "        \"a\",\n",
    "        \"strong\",\n",
    "        \"em\",\n",
    "        \"i\",\n",
    "        \"b\",\n",
    "        \"h1\",\n",
    "        \"h2\",\n",
    "        \"h3\",\n",
    "        \"sup\",\n",
    "        \"strike\",\n",
    "    ]\n",
    "    return any(soup.find(tag) for tag in tags)\n",
    "\n",
    "\n",
    "def percentage_tagged_tokens(sentence):\n",
    "    soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "    total_text = soup.get_text()\n",
    "    total_tokens = len(total_text.split())\n",
    "    tags_to_check = [\n",
    "        \"code\",\n",
    "        \"pre\",\n",
    "        \"a\",\n",
    "        \"strong\",\n",
    "        \"em\",\n",
    "        \"i\",\n",
    "        \"b\",\n",
    "        \"h1\",\n",
    "        \"h2\",\n",
    "        \"h3\",\n",
    "        \"sup\",\n",
    "        \"strike\",\n",
    "    ]\n",
    "    tagged_tokens_set = set()\n",
    "    for tag in tags_to_check:\n",
    "        for element in soup.find_all(tag):\n",
    "            tagged_tokens_set.update(element.get_text().split())\n",
    "    tagged_tokens = len(tagged_tokens_set)\n",
    "    if total_tokens > 0:\n",
    "        return (tagged_tokens / total_tokens) * 100\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def calculate_age(creation_date):\n",
    "    creation_time = datetime.utcfromtimestamp(creation_date)\n",
    "    current_time = datetime.utcnow()\n",
    "    return (current_time - creation_time).days\n",
    "\n",
    "\n",
    "def generate_embedding(new_sentences):\n",
    "    def create_arff_from_list(sentences, filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"@relation sentences\\n\\n\")\n",
    "            f.write(\"@attribute text string\\n\")\n",
    "            f.write(\"@attribute label {positive, negative}\\n\\n\")\n",
    "            f.write(\"@data\\n\")\n",
    "            for sentence in sentences:\n",
    "                cleaned_sentence = sentence.replace(\"'\", \"\\\\'\").replace(\"\\n\", \" \")\n",
    "                f.write(f\"'{cleaned_sentence}',?\\n\")\n",
    "\n",
    "    loaded_filter = read(\"string_to_word_vector.model\")\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "\n",
    "    create_arff_from_list(new_sentences, \"new_sentences.arff\")\n",
    "    new_data = loader.load_file(\"new_sentences.arff\")\n",
    "    new_data.class_is_last()\n",
    "\n",
    "    filter = Filter(\n",
    "        classname=\"weka.filters.unsupervised.attribute.StringToWordVector\",\n",
    "        jobject=loaded_filter,\n",
    "    )\n",
    "    new_vectorized_data = filter.filter(new_data)\n",
    "\n",
    "    new_vectors = []\n",
    "    for i in range(new_vectorized_data.num_instances):\n",
    "        instance = new_vectorized_data.get_instance(i)\n",
    "        vector = [\n",
    "            instance.get_value(j)\n",
    "            for j in range(1, new_vectorized_data.num_attributes - 1)\n",
    "        ]\n",
    "        new_vectors.append(vector)\n",
    "\n",
    "    return new_vectors\n",
    "\n",
    "\n",
    "def sent_tokenize(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def time_conversion(time_string):\n",
    "    time_obj = datetime.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timestamp = int(time_obj.timestamp())\n",
    "    return timestamp\n",
    "\n",
    "\n",
    "def fetch_features(parent, child, user, answer_id, answer_body, api):\n",
    "    child_info = child[child[\"PostId\"] == answer_id]\n",
    "    question_id = child_info[\"ParentId\"].iloc[0]\n",
    "    parent_info = parent[parent[\"PostId\"] == question_id]\n",
    "\n",
    "    child_post_owner = child_info[\"OwnerUserId\"].iloc[0]\n",
    "    parent_post_owner = parent_info[\"OwnerUserId\"].iloc[0]\n",
    "\n",
    "    child_owner_repu = (\n",
    "        0\n",
    "        if pd.isna(child_post_owner)\n",
    "        else user[user[\"UserId\"] == child_post_owner][\"Reputation\"].iloc[0]\n",
    "    )\n",
    "    parent_owner_repu = (\n",
    "        0\n",
    "        if pd.isna(parent_post_owner)\n",
    "        else user[user[\"UserId\"] == parent_post_owner][\"Reputation\"].iloc[0]\n",
    "    )\n",
    "\n",
    "    if pd.isna(child_post_owner):\n",
    "        registered = False\n",
    "    else:\n",
    "        registered = True\n",
    "\n",
    "    question_features = {\n",
    "        \"question_contains_api_element\": (\n",
    "            True\n",
    "            if api in parent_info[\"Title\"].iloc[0].lower()\n",
    "            or api in parent_info[\"Body\"].iloc[0].lower()\n",
    "            else False\n",
    "        ),\n",
    "        \"question_score\": parent_info[\"Score\"].iloc[0],\n",
    "        \"question_views\": parent_info[\"ViewCount\"].iloc[0],\n",
    "        \"question_answer_count\": parent_info[\"AnswerCount\"].iloc[0],\n",
    "        \"question_age\": calculate_age(\n",
    "            time_conversion(parent_info[\"CreationDate\"].iloc[0])\n",
    "        ),\n",
    "        \"question_user_reputation\": parent_owner_repu,\n",
    "    }\n",
    "\n",
    "    if parent_info[\"AcceptedAnswerId\"].iloc[0] == answer_id:\n",
    "        acceptance = True\n",
    "    else:\n",
    "        acceptance = False\n",
    "\n",
    "    answer_features = {\n",
    "        \"answer_score\": child_info[\"Score\"].iloc[0],\n",
    "        \"answer_time_difference\": (\n",
    "            time_conversion(child_info[\"CreationDate\"].iloc[0])\n",
    "            - time_conversion(parent_info[\"CreationDate\"].iloc[0])\n",
    "        )\n",
    "        // 60,\n",
    "        \"answer_size\": len(answer_body),\n",
    "        \"answer_age\": calculate_age(\n",
    "            time_conversion(child_info[\"CreationDate\"].iloc[0])\n",
    "        ),\n",
    "        \"answer_accepted\": acceptance,\n",
    "        \"answer_user_reputation\": child_owner_repu,\n",
    "        \"answer_user_registered\": registered,\n",
    "    }\n",
    "\n",
    "    to_return = dict()\n",
    "    to_return[\"post_id\"] = answer_id\n",
    "    for i in question_features:\n",
    "        to_return[i] = question_features[i]\n",
    "\n",
    "    for i in answer_features:\n",
    "        to_return[i] = answer_features[i]\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PostId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>CreationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54504913</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9171</td>\n",
       "      <td>6</td>\n",
       "      <td>10945616.0</td>\n",
       "      <td>Reading console Char input in Kotlin</td>\n",
       "      <td>&lt;p&gt;I don't know how to get a char as input&lt;/p&gt;...</td>\n",
       "      <td>&lt;kotlin&gt;</td>\n",
       "      <td>2019-02-03 16:19:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PostId  PostTypeId  ParentId  AcceptedAnswerId  Score  ViewCount  \\\n",
       "0  54504913           1       NaN               NaN      0       9171   \n",
       "\n",
       "   AnswerCount  OwnerUserId                                 Title  \\\n",
       "0            6   10945616.0  Reading console Char input in Kotlin   \n",
       "\n",
       "                                                Body      Tags  \\\n",
       "0  <p>I don't know how to get a char as input</p>...  <kotlin>   \n",
       "\n",
       "          CreationDate  \n",
       "0  2019-02-03 16:19:46  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child = pd.read_csv(\"benchmark_csv/post_detail.csv\")\n",
    "parent = pd.read_csv(\"benchmark_csv/parent.csv\")\n",
    "parent.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>Reputation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1968</td>\n",
       "      <td>546223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2684</td>\n",
       "      <td>27246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11612</td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14692</td>\n",
       "      <td>2941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20713</td>\n",
       "      <td>205034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserId  Reputation\n",
       "0    1968      546223\n",
       "1    2684       27246\n",
       "2   11612        4121\n",
       "3   14692        2941\n",
       "4   20713      205034"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = pd.read_csv(\"benchmark_csv/user.csv\")\n",
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       api language knowledge_type   post_id  \\\n",
      "0     AlgorithmConstraints     Java  functionality  74771450   \n",
      "1     AlgorithmConstraints     Java  functionality  72983240   \n",
      "2     AlgorithmConstraints     Java  functionality  76762979   \n",
      "3     AlgorithmConstraints     Java  functionality  74864595   \n",
      "4     AlgorithmConstraints     Java  functionality  75887067   \n",
      "...                    ...      ...            ...       ...   \n",
      "3235             ActionBar  Android    alternative  63978131   \n",
      "3236             ActionBar  Android    alternative  62578531   \n",
      "3237             ActionBar  Android    alternative  76001614   \n",
      "3238             ActionBar  Android    alternative  65551446   \n",
      "3239             ActionBar  Android    alternative  77464305   \n",
      "\n",
      "                                           cleaned_post  \\\n",
      "0     In your case, your input size n = high - low w...   \n",
      "1     You will need to change your domain; from Cost...   \n",
      "2     Normally teams use karate.callSingle() for thi...   \n",
      "3     For your scenario, you can use the Template Me...   \n",
      "4     Fine-tuning the solver is an art on its own. P...   \n",
      "...                                                 ...   \n",
      "3235  There are three ways to get that notification ...   \n",
      "3236  When your second activity goes to foreground, ...   \n",
      "3237  You have created dashboard_menu file and you a...   \n",
      "3238  As written in the google documentation https:/...   \n",
      "3239  android.provider.Settings.ACTION_APPLICATION_D...   \n",
      "\n",
      "                                                   post  \n",
      "0     <p>In your case, your input size <code>n</code...  \n",
      "1     <p>You will need to change your domain; from <...  \n",
      "2     <p>Normally teams use <code>karate.callSingle(...  \n",
      "3     <p>For your scenario, you can use the <a href=...  \n",
      "4     <p>Fine-tuning the solver is an art on its own...  \n",
      "...                                                 ...  \n",
      "3235  <p>There are three ways to get that notificati...  \n",
      "3236  <p>When your second activity goes to foregroun...  \n",
      "3237  <p>You have created <strong>dashboard_menu</st...  \n",
      "3238  <p>As written in the google documentation <a h...  \n",
      "3239  <p><code>android.provider.Settings.ACTION_APPL...  \n",
      "\n",
      "[3240 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find some posts\n",
    "df = pd.read_csv(\"benchmark_csv/retrieved_post.csv\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "if {\"api\", \"knowledge_type\"}.issubset(df.columns):\n",
    "    second_to_fifth_rows = df\n",
    "    print(second_to_fifth_rows)\n",
    "else:\n",
    "    print(\"The columns 'api' and/or 'knowledge_type' do not exist in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences have been split and saved to 'post_sentence_split.csv'.\n"
     ]
    }
   ],
   "source": [
    "sentence_data = []\n",
    "\n",
    "for index, row in second_to_fifth_rows.iterrows():\n",
    "    api = row[\"api\"]\n",
    "    post = row[\"post\"]\n",
    "    post_id = row[\"post_id\"]\n",
    "    doc = nlp(post)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    for sentence in sentences:\n",
    "        sentence_data.append(\n",
    "            {\"api\": api, \"post\": post, \"post_id\": post_id, \"sentence\": sentence}\n",
    "        )\n",
    "\n",
    "sentence_df = pd.DataFrame(sentence_data)\n",
    "\n",
    "sentence_df.to_csv(\"post_sentence_split.csv\", index=False)\n",
    "print(\"Sentences have been split and saved to 'post_sentence_split.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_16028\\4019949538.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(sentence, \"html.parser\")\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_16028\\4019949538.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(sentence, \"html.parser\")\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_16028\\4019949538.py:79: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(sentence, \"html.parser\")\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_16028\\4019949538.py:87: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(sentence, \"html.parser\")\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_16028\\4019949538.py:106: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(sentence, \"html.parser\")\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_16028\\4019949538.py:135: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  creation_time = datetime.utcfromtimestamp(creation_date)\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_16028\\4019949538.py:136: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.utcnow()\n"
     ]
    }
   ],
   "source": [
    "df = sentence_df\n",
    "df[\"num_tokens\"] = df[\"sentence\"].apply(count_tokens)\n",
    "df = is_codeblock(df)\n",
    "df[\"sentence_position\"] = df.groupby(\"post_id\").cumcount() + 1\n",
    "df[\"api_position\"] = df.apply(\n",
    "    lambda row: api_position(row[\"sentence\"], row[\"api\"]), axis=1\n",
    ")\n",
    "df[\"starts_lower_case\"] = df[\"sentence\"].apply(starts_lower_case)\n",
    "df[\"num_code_characters\"] = code_token_count(df)\n",
    "df[\"contains_html_tags\"] = df[\"sentence\"].apply(contains_html_tags)\n",
    "df[\"percentage_tagged_tokens\"] = df[\"sentence\"].apply(percentage_tagged_tokens)\n",
    "\n",
    "parent = pd.read_csv(\"benchmark_csv/parent.csv\")\n",
    "child = pd.read_csv(\"benchmark_csv/post_detail.csv\")\n",
    "user = pd.read_csv(\"benchmark_csv/user.csv\")\n",
    "\n",
    "all_features = []\n",
    "for index, row in df.iterrows():\n",
    "    post_id = row[\"post_id\"]\n",
    "    answer_body = row[\"post\"]\n",
    "    api = row[\"api\"]\n",
    "    features = fetch_features(parent, child, user, post_id, answer_body, api)\n",
    "    all_features.append(features)\n",
    "\n",
    "for i in features:\n",
    "    temp = []\n",
    "    for j in all_features:\n",
    "        temp.append(j[i])\n",
    "    df[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_16028\\1866562912.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(0)\n"
     ]
    }
   ],
   "source": [
    "df[\"sentence\"] = df[\"sentence\"].apply(remove_html_tags)\n",
    "\n",
    "df = df.fillna(0)\n",
    "for col in df.select_dtypes(include=[\"bool\"]).columns:\n",
    "    df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:weka.core.jvm:JVM already running, call jvm.stop() first\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "start()\n",
    "test_sentences = df[\"sentence\"].tolist()  # 提取测试集句子\n",
    "test_embeddings = generate_embedding(test_sentences)  # 使用你的 generate_embedding 函数\n",
    "test_embeddings_df = pd.DataFrame(\n",
    "    test_embeddings, index=df.index\n",
    ")  # 转为 DataFrame，并保持索引一致\n",
    "df = pd.concat(\n",
    "    [df.reset_index(drop=True), test_embeddings_df.reset_index(drop=True)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"official_documents\"\n",
    "\n",
    "txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "\n",
    "official_document_dict = {}\n",
    "\n",
    "for file_path in txt_files:\n",
    "    api = file_path.split(\"\\\\\")[-1][:-4]\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        official_document_dict[api] = sent_tokenize(nlp, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes = []\n",
    "averages = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    api = row[\"api\"]\n",
    "    sentence = row[\"sentence\"]\n",
    "    sents = official_document_dict[api.lower()]\n",
    "    similarities = []\n",
    "    for s in sents:\n",
    "        similarities.append(token_similarity(s, sentence))\n",
    "    maxes.append(max(similarities))\n",
    "    averages.append(np.mean(similarities))\n",
    "\n",
    "df[\"max\"] = maxes\n",
    "df[\"mean\"] = averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_to_be_predicted_by_sise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model loaded successfully.\n",
      "scaler model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "with open(\"svm_model.pkl\", \"rb\") as f:\n",
    "    svm = pickle.load(f)\n",
    "print(\"SVM model loaded successfully.\")\n",
    "\n",
    "with open(\"scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "print(\"scaler model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_columns = [\"api\", \"post\", \"post_id\", \"sentence\"]\n",
    "feature_columns = [col for col in df.columns if col not in info_columns]\n",
    "\n",
    "X_test = df[feature_columns]\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "X_test = X_test[scaler.feature_names_in_]\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prediction\"] = y_pred\n",
    "\n",
    "predicted_ones = df[df[\"prediction\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences for API 'AccessibilityRequestPreparer' written to sise_output\\AccessibilityRequestPreparer.txt.\n",
      "Sentences for API 'ActionBar' written to sise_output\\ActionBar.txt.\n",
      "Sentences for API 'AdapterStateHelper' written to sise_output\\AdapterStateHelper.txt.\n",
      "Sentences for API 'AlgorithmConstraints' written to sise_output\\AlgorithmConstraints.txt.\n",
      "Sentences for API 'Attr' written to sise_output\\Attr.txt.\n",
      "Sentences for API 'Attributes2Impl' written to sise_output\\Attributes2Impl.txt.\n",
      "Sentences for API 'BigDecimal' written to sise_output\\BigDecimal.txt.\n",
      "Sentences for API 'ByteArray' written to sise_output\\ByteArray.txt.\n",
      "Sentences for API 'CharSequence' written to sise_output\\CharSequence.txt.\n",
      "Sentences for API 'CheckBoxPreference' written to sise_output\\CheckBoxPreference.txt.\n",
      "Sentences for API 'CheckpointManager' written to sise_output\\CheckpointManager.txt.\n",
      "Sentences for API 'ConnectionEvent' written to sise_output\\ConnectionEvent.txt.\n",
      "Sentences for API 'DatasetSpec' written to sise_output\\DatasetSpec.txt.\n",
      "Sentences for API 'DistributedIterator' written to sise_output\\DistributedIterator.txt.\n",
      "Sentences for API 'DistributedValues' written to sise_output\\DistributedValues.txt.\n",
      "Sentences for API 'Double' written to sise_output\\Double.txt.\n",
      "Sentences for API 'FragmentManager' written to sise_output\\FragmentManager.txt.\n",
      "Sentences for API 'FragmentTransaction' written to sise_output\\FragmentTransaction.txt.\n",
      "Sentences for API 'GradientTape' written to sise_output\\GradientTape.txt.\n",
      "Sentences for API 'IllegalArgumentException' written to sise_output\\IllegalArgumentException.txt.\n",
      "Sentences for API 'IntArray' written to sise_output\\IntArray.txt.\n",
      "Sentences for API 'LinkedList' written to sise_output\\LinkedList.txt.\n",
      "Sentences for API 'MBeanServer' written to sise_output\\MBeanServer.txt.\n",
      "Sentences for API 'Manifest' written to sise_output\\Manifest.txt.\n",
      "Sentences for API 'MediaPlayer' written to sise_output\\MediaPlayer.txt.\n",
      "Sentences for API 'MediaRouter' written to sise_output\\MediaRouter.txt.\n",
      "Sentences for API 'MessageDigest' written to sise_output\\MessageDigest.txt.\n",
      "Sentences for API 'MirroredStrategy' written to sise_output\\MirroredStrategy.txt.\n",
      "Sentences for API 'Model' written to sise_output\\Model.txt.\n",
      "Sentences for API 'MouseListener' written to sise_output\\MouseListener.txt.\n",
      "Sentences for API 'NotificationManager' written to sise_output\\NotificationManager.txt.\n",
      "Sentences for API 'RaggedTensorSpec' written to sise_output\\RaggedTensorSpec.txt.\n",
      "Sentences for API 'SAXParser' written to sise_output\\SAXParser.txt.\n",
      "Sentences for API 'SparseTensorSpec' written to sise_output\\SparseTensorSpec.txt.\n",
      "Sentences for API 'TensorSpec' written to sise_output\\TensorSpec.txt.\n",
      "Sentences for API 'TreeMap' written to sise_output\\TreeMap.txt.\n",
      "Sentences for API 'URISyntaxException' written to sise_output\\URISyntaxException.txt.\n",
      "Sentences for API 'UUID' written to sise_output\\UUID.txt.\n",
      "Sentences for API 'VariableElement' written to sise_output\\VariableElement.txt.\n",
      "Sentences for API 'VariableSynchronization' written to sise_output\\VariableSynchronization.txt.\n",
      "Sentences for API 'X509TrustManager' written to sise_output\\X509TrustManager.txt.\n",
      "Sentences for API 'charArrayOf' written to sise_output\\charArrayOf.txt.\n",
      "Sentences for API 'name_scope' written to sise_output\\name_scope.txt.\n",
      "Sentences for API 'synchronized' written to sise_output\\synchronized.txt.\n",
      "Sentences for API 'ubyteArrayOf' written to sise_output\\ubyteArrayOf.txt.\n",
      "Sentences for API 'uintArrayOf' written to sise_output\\uintArrayOf.txt.\n",
      "Sentences for API 'ulongArrayOf' written to sise_output\\ulongArrayOf.txt.\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"sise_output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for api, group in predicted_ones.groupby(\"api\"):\n",
    "    file_path = os.path.join(output_folder, f\"{api}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\nlonglivebonankou\\n\".join(list(set(group[\"sentence\"]))))\n",
    "        print(f\"Sentences for API '{api}' written to {file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5986"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       If you're going to use the simplest JMS patter...\n",
       "31      If your design depends on what the observers n...\n",
       "35      The consumer's side is more likely to be changed.\n",
       "43      In other words, the broker must dispatch messa...\n",
       "54             I couldn't see your messageConverter bean.\n",
       "                              ...                        \n",
       "5950    ByteArray.\\nOn Kotlin/JVM, ByteArray compiles ...\n",
       "5966    /api/latest/jvm/stdlib/kotlin.text/to-byte-arr...\n",
       "5980    It works for an empty ByteArray, as well as on...\n",
       "5983    It can be used as follows:\\n@OptIn(Experimenta...\n",
       "5985    ByteArray = \\n    ByteArray (size) {i -&gt; (d...\n",
       "Name: sentence, Length: 596, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ones[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_ones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
