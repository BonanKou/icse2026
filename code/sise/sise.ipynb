{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import weka.core.jvm as jvm\n",
    "from weka.core.dataset import Instances, Instance\n",
    "from weka.filters import Filter\n",
    "from weka.core.converters import Loader, Saver\n",
    "import pandas as pd\n",
    "from weka.core.converters import Loader\n",
    "from weka.filters import Filter\n",
    "from weka.core.serialization import write, read\n",
    "from weka.core.jvm import start, stop\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(sentence):\n",
    "    soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    text = str(text)\n",
    "    return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "\n",
    "def token_similarity(string1, string2):\n",
    "    def tokenize(string):\n",
    "        return set(string.split())\n",
    "\n",
    "    tokens_a = tokenize(string1)\n",
    "    tokens_b = tokenize(string2)\n",
    "    intersection = len(tokens_a & tokens_b)\n",
    "    magnitude_a = math.sqrt(len(tokens_a))\n",
    "    magnitude_b = math.sqrt(len(tokens_b))\n",
    "    if magnitude_a == 0 or magnitude_b == 0:\n",
    "        return 0.0\n",
    "    similarity = intersection / (magnitude_a * magnitude_b)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def count_tokens(sentence):\n",
    "    sentence = remove_html(sentence)\n",
    "    return len(sentence.split())\n",
    "\n",
    "\n",
    "def is_codeblock(df):\n",
    "    inside_pre = False\n",
    "    pre_sentences = []\n",
    "    for sentence in df[\"sentence\"]:\n",
    "        if \"<pre>\" in sentence:\n",
    "            inside_pre = True\n",
    "        if inside_pre:\n",
    "            pre_sentences.append(1)\n",
    "        else:\n",
    "            pre_sentences.append(0)\n",
    "        if \"</pre>\" in sentence:\n",
    "            inside_pre = False\n",
    "    df[\"is_codeblock\"] = pre_sentences\n",
    "    return df\n",
    "\n",
    "\n",
    "def api_position(sentence, api_name):\n",
    "    position = sentence.find(api_name)\n",
    "    return position + 1 if position != -1 else 0\n",
    "\n",
    "\n",
    "def starts_lower_case(sentence):\n",
    "    sentence = remove_html(sentence)\n",
    "    return 1 if sentence and sentence[0].islower() else 0\n",
    "\n",
    "\n",
    "def code_token_count(df):\n",
    "    code_token = []\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row[\"sentence\"]\n",
    "        is_codeblock = row[\"is_codeblock\"]\n",
    "        cleaned_sentence = remove_html(sentence)\n",
    "        if is_codeblock:\n",
    "            total_length = len(sentence)\n",
    "            pre_start = sentence.find(\"<pre>\")\n",
    "            pre_end = sentence.find(\"</pre>\")\n",
    "            if pre_start == -1 and pre_end == -1:\n",
    "                code_token.append(total_length)\n",
    "            elif pre_start == -1:\n",
    "                code_token.append(pre_end)\n",
    "            elif pre_end == -1:\n",
    "                code_token.append(total_length - pre_start)\n",
    "            else:\n",
    "                soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "                code_elements = soup.find_all([\"pre\", \"code\"])\n",
    "                token_num = sum(len(code.get_text()) for code in code_elements)\n",
    "                code_token.append(token_num)\n",
    "        else:\n",
    "            soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "            code_elements = soup.find_all([\"pre\", \"code\"])\n",
    "            token_num = sum(len(code.get_text()) for code in code_elements)\n",
    "            code_token.append(token_num)\n",
    "    df[\"num_code_characters\"] = code_token\n",
    "\n",
    "\n",
    "def contains_html_tags(sentence):\n",
    "    soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "    tags = [\n",
    "        \"code\",\n",
    "        \"pre\",\n",
    "        \"a\",\n",
    "        \"strong\",\n",
    "        \"em\",\n",
    "        \"i\",\n",
    "        \"b\",\n",
    "        \"h1\",\n",
    "        \"h2\",\n",
    "        \"h3\",\n",
    "        \"sup\",\n",
    "        \"strike\",\n",
    "    ]\n",
    "    return any(soup.find(tag) for tag in tags)\n",
    "\n",
    "\n",
    "def percentage_tagged_tokens(sentence):\n",
    "    soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "    total_text = soup.get_text()\n",
    "    total_tokens = len(total_text.split())\n",
    "    tags_to_check = [\n",
    "        \"code\",\n",
    "        \"pre\",\n",
    "        \"a\",\n",
    "        \"strong\",\n",
    "        \"em\",\n",
    "        \"i\",\n",
    "        \"b\",\n",
    "        \"h1\",\n",
    "        \"h2\",\n",
    "        \"h3\",\n",
    "        \"sup\",\n",
    "        \"strike\",\n",
    "    ]\n",
    "    tagged_tokens_set = set()\n",
    "    for tag in tags_to_check:\n",
    "        for element in soup.find_all(tag):\n",
    "            tagged_tokens_set.update(element.get_text().split())\n",
    "    tagged_tokens = len(tagged_tokens_set)\n",
    "    if total_tokens > 0:\n",
    "        return (tagged_tokens / total_tokens) * 100\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def calculate_age(creation_date):\n",
    "    creation_time = datetime.utcfromtimestamp(creation_date)\n",
    "    current_time = datetime.utcnow()\n",
    "    return (current_time - creation_time).days\n",
    "\n",
    "\n",
    "def generate_embedding(new_sentences):\n",
    "    def create_arff_from_list(sentences, filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"@relation sentences\\n\\n\")\n",
    "            f.write(\"@attribute text string\\n\")\n",
    "            f.write(\"@attribute label {positive, negative}\\n\\n\")\n",
    "            f.write(\"@data\\n\")\n",
    "            for sentence in sentences:\n",
    "                cleaned_sentence = sentence.replace(\"'\", \"\\\\'\").replace(\"\\n\", \" \")\n",
    "                f.write(f\"'{cleaned_sentence}',?\\n\")\n",
    "\n",
    "    loaded_filter = read(\"string_to_word_vector.model\")\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "\n",
    "    create_arff_from_list(new_sentences, \"new_sentences.arff\")\n",
    "    new_data = loader.load_file(\"new_sentences.arff\")\n",
    "    new_data.class_is_last()\n",
    "\n",
    "    filter = Filter(\n",
    "        classname=\"weka.filters.unsupervised.attribute.StringToWordVector\",\n",
    "        jobject=loaded_filter,\n",
    "    )\n",
    "    new_vectorized_data = filter.filter(new_data)\n",
    "\n",
    "    new_vectors = []\n",
    "    for i in range(new_vectorized_data.num_instances):\n",
    "        instance = new_vectorized_data.get_instance(i)\n",
    "        vector = [\n",
    "            instance.get_value(j)\n",
    "            for j in range(1, new_vectorized_data.num_attributes - 1)\n",
    "        ]\n",
    "        new_vectors.append(vector)\n",
    "\n",
    "    return new_vectors\n",
    "\n",
    "\n",
    "def sent_tokenize(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def time_conversion(time_string):\n",
    "    time_obj = datetime.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timestamp = int(time_obj.timestamp())\n",
    "    return timestamp\n",
    "\n",
    "\n",
    "def fetch_features(parent, child, user, answer_id, answer_body, api):\n",
    "    child_info = child[child[\"PostId\"] == answer_id]\n",
    "    question_id = child_info[\"ParentId\"].iloc[0]\n",
    "    parent_info = parent[parent[\"PostId\"] == question_id]\n",
    "\n",
    "    child_post_owner = child_info[\"OwnerUserId\"].iloc[0]\n",
    "    parent_post_owner = parent_info[\"OwnerUserId\"].iloc[0]\n",
    "\n",
    "    child_owner_repu = (\n",
    "        0\n",
    "        if pd.isna(child_post_owner)\n",
    "        else user[user[\"UserId\"] == child_post_owner][\"Reputation\"].iloc[0]\n",
    "    )\n",
    "    parent_owner_repu = (\n",
    "        0\n",
    "        if pd.isna(parent_post_owner)\n",
    "        else user[user[\"UserId\"] == parent_post_owner][\"Reputation\"].iloc[0]\n",
    "    )\n",
    "\n",
    "    if pd.isna(child_post_owner):\n",
    "        registered = False\n",
    "    else:\n",
    "        registered = True\n",
    "\n",
    "    question_features = {\n",
    "        \"question_contains_api_element\": (\n",
    "            True\n",
    "            if api in parent_info[\"Title\"].iloc[0].lower()\n",
    "            or api in parent_info[\"Body\"].iloc[0].lower()\n",
    "            else False\n",
    "        ),\n",
    "        \"question_score\": parent_info[\"Score\"].iloc[0],\n",
    "        \"question_views\": parent_info[\"ViewCount\"].iloc[0],\n",
    "        \"question_answer_count\": parent_info[\"AnswerCount\"].iloc[0],\n",
    "        \"question_age\": calculate_age(\n",
    "            time_conversion(parent_info[\"CreationDate\"].iloc[0])\n",
    "        ),\n",
    "        \"question_user_reputation\": parent_owner_repu,\n",
    "    }\n",
    "\n",
    "    if parent_info[\"AcceptedAnswerId\"].iloc[0] == answer_id:\n",
    "        acceptance = True\n",
    "    else:\n",
    "        acceptance = False\n",
    "\n",
    "    answer_features = {\n",
    "        \"answer_score\": child_info[\"Score\"].iloc[0],\n",
    "        \"answer_time_difference\": (\n",
    "            time_conversion(child_info[\"CreationDate\"].iloc[0])\n",
    "            - time_conversion(parent_info[\"CreationDate\"].iloc[0])\n",
    "        )\n",
    "        // 60,\n",
    "        \"answer_size\": len(answer_body),\n",
    "        \"answer_age\": calculate_age(\n",
    "            time_conversion(child_info[\"CreationDate\"].iloc[0])\n",
    "        ),\n",
    "        \"answer_accepted\": acceptance,\n",
    "        \"answer_user_reputation\": child_owner_repu,\n",
    "        \"answer_user_registered\": registered,\n",
    "    }\n",
    "\n",
    "    to_return = dict()\n",
    "    to_return[\"post_id\"] = answer_id\n",
    "    for i in question_features:\n",
    "        to_return[i] = question_features[i]\n",
    "\n",
    "    for i in answer_features:\n",
    "        to_return[i] = answer_features[i]\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child = pd.read_csv(\"benchmark_csv/post_detail.csv\")\n",
    "parent = pd.read_csv(\"benchmark_csv/parent.csv\")\n",
    "parent.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = pd.read_csv(\"benchmark_csv/user.csv\")\n",
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find some posts\n",
    "df = pd.read_csv(\"benchmark_csv/retrieved_post.csv\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "if {\"api\", \"knowledge_type\"}.issubset(df.columns):\n",
    "    second_to_fifth_rows = df\n",
    "    print(second_to_fifth_rows)\n",
    "else:\n",
    "    print(\"The columns 'api' and/or 'knowledge_type' do not exist in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences have been split and saved to 'post_sentence_split.csv'.\n"
     ]
    }
   ],
   "source": [
    "sentence_data = []\n",
    "\n",
    "for index, row in second_to_fifth_rows.iterrows():\n",
    "    api = row[\"api\"]\n",
    "    post = row[\"post\"]\n",
    "    post_id = row[\"post_id\"]\n",
    "    doc = nlp(post)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    for sentence in sentences:\n",
    "        sentence_data.append(\n",
    "            {\"api\": api, \"post\": post, \"post_id\": post_id, \"sentence\": sentence}\n",
    "        )\n",
    "\n",
    "sentence_df = pd.DataFrame(sentence_data)\n",
    "\n",
    "sentence_df.to_csv(\"post_sentence_split.csv\", index=False)\n",
    "print(\"Sentences have been split and saved to 'post_sentence_split.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sentence_df\n",
    "df[\"num_tokens\"] = df[\"sentence\"].apply(count_tokens)\n",
    "df = is_codeblock(df)\n",
    "df[\"sentence_position\"] = df.groupby(\"post_id\").cumcount() + 1\n",
    "df[\"api_position\"] = df.apply(\n",
    "    lambda row: api_position(row[\"sentence\"], row[\"api\"]), axis=1\n",
    ")\n",
    "df[\"starts_lower_case\"] = df[\"sentence\"].apply(starts_lower_case)\n",
    "df[\"num_code_characters\"] = code_token_count(df)\n",
    "df[\"contains_html_tags\"] = df[\"sentence\"].apply(contains_html_tags)\n",
    "df[\"percentage_tagged_tokens\"] = df[\"sentence\"].apply(percentage_tagged_tokens)\n",
    "\n",
    "parent = pd.read_csv(\"benchmark_csv/parent.csv\")\n",
    "child = pd.read_csv(\"benchmark_csv/post_detail.csv\")\n",
    "user = pd.read_csv(\"benchmark_csv/user.csv\")\n",
    "\n",
    "all_features = []\n",
    "for index, row in df.iterrows():\n",
    "    post_id = row[\"post_id\"]\n",
    "    answer_body = row[\"post\"]\n",
    "    api = row[\"api\"]\n",
    "    features = fetch_features(parent, child, user, post_id, answer_body, api)\n",
    "    all_features.append(features)\n",
    "\n",
    "for i in features:\n",
    "    temp = []\n",
    "    for j in all_features:\n",
    "        temp.append(j[i])\n",
    "    df[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentence\"] = df[\"sentence\"].apply(remove_html_tags)\n",
    "\n",
    "df = df.fillna(0)\n",
    "for col in df.select_dtypes(include=[\"bool\"]).columns:\n",
    "    df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start()\n",
    "test_sentences = df[\"sentence\"].tolist()\n",
    "test_embeddings = generate_embedding(test_sentences)\n",
    "test_embeddings_df = pd.DataFrame(test_embeddings, index=df.index)\n",
    "df = pd.concat(\n",
    "    [df.reset_index(drop=True), test_embeddings_df.reset_index(drop=True)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"official_documents\"\n",
    "\n",
    "txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "\n",
    "official_document_dict = {}\n",
    "\n",
    "for file_path in txt_files:\n",
    "    api = file_path.split(\"\\\\\")[-1][:-4]\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        official_document_dict[api] = sent_tokenize(nlp, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes = []\n",
    "averages = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    api = row[\"api\"]\n",
    "    sentence = row[\"sentence\"]\n",
    "    sents = official_document_dict[api.lower()]\n",
    "    similarities = []\n",
    "    for s in sents:\n",
    "        similarities.append(token_similarity(s, sentence))\n",
    "    maxes.append(max(similarities))\n",
    "    averages.append(np.mean(similarities))\n",
    "\n",
    "df[\"max\"] = maxes\n",
    "df[\"mean\"] = averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_to_be_predicted_by_sise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model loaded successfully.\n",
      "scaler model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "with open(\"svm_model.pkl\", \"rb\") as f:\n",
    "    svm = pickle.load(f)\n",
    "print(\"SVM model loaded successfully.\")\n",
    "\n",
    "with open(\"scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "print(\"scaler model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_columns = [\"api\", \"post\", \"post_id\", \"sentence\"]\n",
    "feature_columns = [col for col in df.columns if col not in info_columns]\n",
    "\n",
    "X_test = df[feature_columns]\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "X_test = X_test[scaler.feature_names_in_]\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prediction\"] = y_pred\n",
    "\n",
    "predicted_ones = df[df[\"prediction\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences for API 'AccessibilityRequestPreparer' written to sise_output\\AccessibilityRequestPreparer.txt.\n",
      "Sentences for API 'ActionBar' written to sise_output\\ActionBar.txt.\n",
      "Sentences for API 'AdapterStateHelper' written to sise_output\\AdapterStateHelper.txt.\n",
      "Sentences for API 'AlgorithmConstraints' written to sise_output\\AlgorithmConstraints.txt.\n",
      "Sentences for API 'Attr' written to sise_output\\Attr.txt.\n",
      "Sentences for API 'Attributes2Impl' written to sise_output\\Attributes2Impl.txt.\n",
      "Sentences for API 'BigDecimal' written to sise_output\\BigDecimal.txt.\n",
      "Sentences for API 'ByteArray' written to sise_output\\ByteArray.txt.\n",
      "Sentences for API 'CharSequence' written to sise_output\\CharSequence.txt.\n",
      "Sentences for API 'CheckBoxPreference' written to sise_output\\CheckBoxPreference.txt.\n",
      "Sentences for API 'CheckpointManager' written to sise_output\\CheckpointManager.txt.\n",
      "Sentences for API 'ConnectionEvent' written to sise_output\\ConnectionEvent.txt.\n",
      "Sentences for API 'DatasetSpec' written to sise_output\\DatasetSpec.txt.\n",
      "Sentences for API 'DistributedIterator' written to sise_output\\DistributedIterator.txt.\n",
      "Sentences for API 'DistributedValues' written to sise_output\\DistributedValues.txt.\n",
      "Sentences for API 'Double' written to sise_output\\Double.txt.\n",
      "Sentences for API 'FragmentManager' written to sise_output\\FragmentManager.txt.\n",
      "Sentences for API 'FragmentTransaction' written to sise_output\\FragmentTransaction.txt.\n",
      "Sentences for API 'GradientTape' written to sise_output\\GradientTape.txt.\n",
      "Sentences for API 'IllegalArgumentException' written to sise_output\\IllegalArgumentException.txt.\n",
      "Sentences for API 'IntArray' written to sise_output\\IntArray.txt.\n",
      "Sentences for API 'LinkedList' written to sise_output\\LinkedList.txt.\n",
      "Sentences for API 'MBeanServer' written to sise_output\\MBeanServer.txt.\n",
      "Sentences for API 'Manifest' written to sise_output\\Manifest.txt.\n",
      "Sentences for API 'MediaPlayer' written to sise_output\\MediaPlayer.txt.\n",
      "Sentences for API 'MediaRouter' written to sise_output\\MediaRouter.txt.\n",
      "Sentences for API 'MessageDigest' written to sise_output\\MessageDigest.txt.\n",
      "Sentences for API 'MirroredStrategy' written to sise_output\\MirroredStrategy.txt.\n",
      "Sentences for API 'Model' written to sise_output\\Model.txt.\n",
      "Sentences for API 'MouseListener' written to sise_output\\MouseListener.txt.\n",
      "Sentences for API 'NotificationManager' written to sise_output\\NotificationManager.txt.\n",
      "Sentences for API 'RaggedTensorSpec' written to sise_output\\RaggedTensorSpec.txt.\n",
      "Sentences for API 'SAXParser' written to sise_output\\SAXParser.txt.\n",
      "Sentences for API 'SparseTensorSpec' written to sise_output\\SparseTensorSpec.txt.\n",
      "Sentences for API 'TensorSpec' written to sise_output\\TensorSpec.txt.\n",
      "Sentences for API 'TreeMap' written to sise_output\\TreeMap.txt.\n",
      "Sentences for API 'URISyntaxException' written to sise_output\\URISyntaxException.txt.\n",
      "Sentences for API 'UUID' written to sise_output\\UUID.txt.\n",
      "Sentences for API 'VariableElement' written to sise_output\\VariableElement.txt.\n",
      "Sentences for API 'VariableSynchronization' written to sise_output\\VariableSynchronization.txt.\n",
      "Sentences for API 'X509TrustManager' written to sise_output\\X509TrustManager.txt.\n",
      "Sentences for API 'charArrayOf' written to sise_output\\charArrayOf.txt.\n",
      "Sentences for API 'name_scope' written to sise_output\\name_scope.txt.\n",
      "Sentences for API 'synchronized' written to sise_output\\synchronized.txt.\n",
      "Sentences for API 'ubyteArrayOf' written to sise_output\\ubyteArrayOf.txt.\n",
      "Sentences for API 'uintArrayOf' written to sise_output\\uintArrayOf.txt.\n",
      "Sentences for API 'ulongArrayOf' written to sise_output\\ulongArrayOf.txt.\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"sise_output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for api, group in predicted_ones.groupby(\"api\"):\n",
    "    file_path = os.path.join(output_folder, f\"{api}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\nlonglivebonankou\\n\".join(list(set(group[\"sentence\"]))))\n",
    "        print(f\"Sentences for API '{api}' written to {file_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
