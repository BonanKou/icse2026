{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import weka.core.jvm as jvm\n",
    "from weka.core.dataset import Instances, Instance\n",
    "from weka.filters import Filter\n",
    "from weka.core.converters import Loader, Saver\n",
    "import pandas as pd\n",
    "from weka.core.converters import Loader\n",
    "from weka.filters import Filter\n",
    "from weka.core.serialization import write, read\n",
    "from weka.core.jvm import start, stop\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(sentence):\n",
    "    soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    text = str(text)\n",
    "    return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "\n",
    "def token_similarity(string1, string2):\n",
    "    def tokenize(string):\n",
    "        return set(string.split())\n",
    "\n",
    "    tokens_a = tokenize(string1)\n",
    "    tokens_b = tokenize(string2)\n",
    "    intersection = len(tokens_a & tokens_b)\n",
    "    magnitude_a = math.sqrt(len(tokens_a))\n",
    "    magnitude_b = math.sqrt(len(tokens_b))\n",
    "    if magnitude_a == 0 or magnitude_b == 0:\n",
    "        return 0.0\n",
    "    similarity = intersection / (magnitude_a * magnitude_b)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def count_tokens(sentence):\n",
    "    sentence = remove_html(sentence)\n",
    "    return len(sentence.split())\n",
    "\n",
    "\n",
    "def is_codeblock(df):\n",
    "    inside_pre = False\n",
    "    pre_sentences = []\n",
    "    for sentence in df[\"sentence\"]:\n",
    "        if \"<pre>\" in sentence:\n",
    "            inside_pre = True\n",
    "        if inside_pre:\n",
    "            pre_sentences.append(1)\n",
    "        else:\n",
    "            pre_sentences.append(0)\n",
    "        if \"</pre>\" in sentence:\n",
    "            inside_pre = False\n",
    "    df[\"is_codeblock\"] = pre_sentences\n",
    "    return df\n",
    "\n",
    "\n",
    "def api_position(sentence, api_name):\n",
    "    position = sentence.find(api_name)\n",
    "    return position + 1 if position != -1 else 0\n",
    "\n",
    "\n",
    "def starts_lower_case(sentence):\n",
    "    sentence = remove_html(sentence)\n",
    "    return 1 if sentence and sentence[0].islower() else 0\n",
    "\n",
    "\n",
    "def code_token_count(df):\n",
    "    code_token = []\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row[\"sentence\"]\n",
    "        is_codeblock = row[\"is_codeblock\"]\n",
    "        cleaned_sentence = remove_html(sentence)\n",
    "        if is_codeblock:\n",
    "            total_length = len(sentence)\n",
    "            pre_start = sentence.find(\"<pre>\")\n",
    "            pre_end = sentence.find(\"</pre>\")\n",
    "            if pre_start == -1 and pre_end == -1:\n",
    "                code_token.append(total_length)\n",
    "            elif pre_start == -1:\n",
    "                code_token.append(pre_end)\n",
    "            elif pre_end == -1:\n",
    "                code_token.append(total_length - pre_start)\n",
    "            else:\n",
    "                soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "                code_elements = soup.find_all([\"pre\", \"code\"])\n",
    "                token_num = sum(len(code.get_text()) for code in code_elements)\n",
    "                code_token.append(token_num)\n",
    "        else:\n",
    "            soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "            code_elements = soup.find_all([\"pre\", \"code\"])\n",
    "            token_num = sum(len(code.get_text()) for code in code_elements)\n",
    "            code_token.append(token_num)\n",
    "    df[\"num_code_characters\"] = code_token\n",
    "\n",
    "\n",
    "def contains_html_tags(sentence):\n",
    "    soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "    tags = [\n",
    "        \"code\",\n",
    "        \"pre\",\n",
    "        \"a\",\n",
    "        \"strong\",\n",
    "        \"em\",\n",
    "        \"i\",\n",
    "        \"b\",\n",
    "        \"h1\",\n",
    "        \"h2\",\n",
    "        \"h3\",\n",
    "        \"sup\",\n",
    "        \"strike\",\n",
    "    ]\n",
    "    return any(soup.find(tag) for tag in tags)\n",
    "\n",
    "\n",
    "def percentage_tagged_tokens(sentence):\n",
    "    soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "    total_text = soup.get_text()\n",
    "    total_tokens = len(total_text.split())\n",
    "    tags_to_check = [\n",
    "        \"code\",\n",
    "        \"pre\",\n",
    "        \"a\",\n",
    "        \"strong\",\n",
    "        \"em\",\n",
    "        \"i\",\n",
    "        \"b\",\n",
    "        \"h1\",\n",
    "        \"h2\",\n",
    "        \"h3\",\n",
    "        \"sup\",\n",
    "        \"strike\",\n",
    "    ]\n",
    "    tagged_tokens_set = set()\n",
    "    for tag in tags_to_check:\n",
    "        for element in soup.find_all(tag):\n",
    "            tagged_tokens_set.update(element.get_text().split())\n",
    "    tagged_tokens = len(tagged_tokens_set)\n",
    "    if total_tokens > 0:\n",
    "        return (tagged_tokens / total_tokens) * 100\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def calculate_age(creation_date):\n",
    "    creation_time = datetime.utcfromtimestamp(creation_date)\n",
    "    current_time = datetime.utcnow()\n",
    "    return (current_time - creation_time).days\n",
    "\n",
    "\n",
    "def generate_embedding(new_sentences):\n",
    "    def create_arff_from_list(sentences, filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"@relation sentences\\n\\n\")\n",
    "            f.write(\"@attribute text string\\n\")\n",
    "            f.write(\"@attribute label {positive, negative}\\n\\n\")\n",
    "            f.write(\"@data\\n\")\n",
    "            for sentence in sentences:\n",
    "                cleaned_sentence = sentence.replace(\"'\", \"\\\\'\").replace(\"\\n\", \" \")\n",
    "                f.write(f\"'{cleaned_sentence}',?\\n\")\n",
    "\n",
    "    loaded_filter = read(\"string_to_word_vector.model\")\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "\n",
    "    create_arff_from_list(new_sentences, \"new_sentences.arff\")\n",
    "    new_data = loader.load_file(\"new_sentences.arff\")\n",
    "    new_data.class_is_last()\n",
    "\n",
    "    filter = Filter(\n",
    "        classname=\"weka.filters.unsupervised.attribute.StringToWordVector\",\n",
    "        jobject=loaded_filter,\n",
    "    )\n",
    "    new_vectorized_data = filter.filter(new_data)\n",
    "\n",
    "    new_vectors = []\n",
    "    for i in range(new_vectorized_data.num_instances):\n",
    "        instance = new_vectorized_data.get_instance(i)\n",
    "        vector = [\n",
    "            instance.get_value(j)\n",
    "            for j in range(1, new_vectorized_data.num_attributes - 1)\n",
    "        ]\n",
    "        new_vectors.append(vector)\n",
    "\n",
    "    return new_vectors\n",
    "\n",
    "\n",
    "def sent_tokenize(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def time_conversion(time_string):\n",
    "    time_obj = datetime.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timestamp = int(time_obj.timestamp())\n",
    "    return timestamp\n",
    "\n",
    "\n",
    "def fetch_features(parent, child, user, answer_id, answer_body, api):\n",
    "    child_info = child[child[\"PostId\"] == answer_id]\n",
    "    question_id = child_info[\"ParentId\"].iloc[0]\n",
    "    parent_info = parent[parent[\"PostId\"] == question_id]\n",
    "\n",
    "    child_post_owner = child_info[\"OwnerUserId\"].iloc[0]\n",
    "    parent_post_owner = parent_info[\"OwnerUserId\"].iloc[0]\n",
    "\n",
    "    child_owner_repu = (\n",
    "        0\n",
    "        if pd.isna(child_post_owner)\n",
    "        else user[user[\"UserId\"] == child_post_owner][\"Reputation\"].iloc[0]\n",
    "    )\n",
    "    parent_owner_repu = (\n",
    "        0\n",
    "        if pd.isna(parent_post_owner)\n",
    "        else user[user[\"UserId\"] == parent_post_owner][\"Reputation\"].iloc[0]\n",
    "    )\n",
    "\n",
    "    if pd.isna(child_post_owner):\n",
    "        registered = False\n",
    "    else:\n",
    "        registered = True\n",
    "\n",
    "    question_features = {\n",
    "        \"question_contains_api_element\": (\n",
    "            True\n",
    "            if api in parent_info[\"Title\"].iloc[0].lower()\n",
    "            or api in parent_info[\"Body\"].iloc[0].lower()\n",
    "            else False\n",
    "        ),\n",
    "        \"question_score\": parent_info[\"Score\"].iloc[0],\n",
    "        \"question_views\": parent_info[\"ViewCount\"].iloc[0],\n",
    "        \"question_answer_count\": parent_info[\"AnswerCount\"].iloc[0],\n",
    "        \"question_age\": calculate_age(\n",
    "            time_conversion(parent_info[\"CreationDate\"].iloc[0])\n",
    "        ),\n",
    "        \"question_user_reputation\": parent_owner_repu,\n",
    "    }\n",
    "\n",
    "    if parent_info[\"AcceptedAnswerId\"].iloc[0] == answer_id:\n",
    "        acceptance = True\n",
    "    else:\n",
    "        acceptance = False\n",
    "\n",
    "    answer_features = {\n",
    "        \"answer_score\": child_info[\"Score\"].iloc[0],\n",
    "        \"answer_time_difference\": (\n",
    "            time_conversion(child_info[\"CreationDate\"].iloc[0])\n",
    "            - time_conversion(parent_info[\"CreationDate\"].iloc[0])\n",
    "        )\n",
    "        // 60,\n",
    "        \"answer_size\": len(answer_body),\n",
    "        \"answer_age\": calculate_age(\n",
    "            time_conversion(child_info[\"CreationDate\"].iloc[0])\n",
    "        ),\n",
    "        \"answer_accepted\": acceptance,\n",
    "        \"answer_user_reputation\": child_owner_repu,\n",
    "        \"answer_user_registered\": registered,\n",
    "    }\n",
    "\n",
    "    to_return = dict()\n",
    "    to_return[\"post_id\"] = answer_id\n",
    "    for i in question_features:\n",
    "        to_return[i] = question_features[i]\n",
    "\n",
    "    for i in answer_features:\n",
    "        to_return[i] = answer_features[i]\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PostId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>CreationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77738225</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77763064.0</td>\n",
       "      <td>2</td>\n",
       "      <td>362</td>\n",
       "      <td>2</td>\n",
       "      <td>429377.0</td>\n",
       "      <td>@Transactional doesn't work with @Async in Web...</td>\n",
       "      <td>&lt;p&gt;I am trying to use &lt;code&gt;@Transactional&lt;/co...</td>\n",
       "      <td>&lt;java&gt;&lt;spring&gt;&lt;spring-boot&gt;&lt;spring-data-jpa&gt;&lt;w...</td>\n",
       "      <td>2023-12-31 04:36:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PostId  PostTypeId  ParentId  AcceptedAnswerId  Score  ViewCount  \\\n",
       "0  77738225           1       NaN        77763064.0      2        362   \n",
       "\n",
       "   AnswerCount  OwnerUserId  \\\n",
       "0            2     429377.0   \n",
       "\n",
       "                                               Title  \\\n",
       "0  @Transactional doesn't work with @Async in Web...   \n",
       "\n",
       "                                                Body  \\\n",
       "0  <p>I am trying to use <code>@Transactional</co...   \n",
       "\n",
       "                                                Tags         CreationDate  \n",
       "0  <java><spring><spring-boot><spring-data-jpa><w...  2023-12-31 04:36:30  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child = pd.read_csv(\"details_of_retrieved_posts.csv\")\n",
    "parent = pd.read_csv(\"parentpost.csv\")\n",
    "parent.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>Reputation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22683654</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22623172</td>\n",
       "      <td>1884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22504890</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22455420</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22385606</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     UserId  Reputation\n",
       "0  22683654         104\n",
       "1  22623172        1884\n",
       "2  22504890          43\n",
       "3  22455420          36\n",
       "4  22385606          15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = pd.read_csv(\"users.csv\")\n",
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               api language knowledge_type   post_id  \\\n",
      "0    MessageDigest     Java  functionality  74682875   \n",
      "1    MessageDigest     Java  functionality  75890236   \n",
      "2    MessageDigest     Java  functionality  74448585   \n",
      "3    MessageDigest     Java  functionality  77230698   \n",
      "4    MessageDigest     Java  functionality  75892592   \n",
      "..             ...      ...            ...       ...   \n",
      "835      ByteArray   Kotlin    alternative  64852886   \n",
      "836      ByteArray   Kotlin    alternative  55416646   \n",
      "837      ByteArray   Kotlin    alternative  65199270   \n",
      "838      ByteArray   Kotlin    alternative  76830156   \n",
      "839      ByteArray   Kotlin    alternative  69891559   \n",
      "\n",
      "                                                  post  \\\n",
      "0    <p>If you're going to use the simplest JMS pat...   \n",
      "1    <p>First of all - MessageId is array of 24 byt...   \n",
      "2    <p>If <code>message1</code> and <code>message2...   \n",
      "3    <p>You can use inner class for MessageSender:<...   \n",
      "4    <p>How does Log4j 2.x interpret the <code>(Str...   \n",
      "..                                                 ...   \n",
      "835  <p>The following is an object serializable cla...   \n",
      "836  <p>The easiest way is to use </p>\\n\\n<pre><cod...   \n",
      "837  <p>The easiest way to make a <code>ByteArray</...   \n",
      "838  <p>Kotlin 1.9 introduced experimental <a href=...   \n",
      "839  <p>Here is an one liner that will give you a B...   \n",
      "\n",
      "                                                 label  contain_knowledge  \n",
      "0    No. The post does not contain functionality kn...                  0  \n",
      "1    No. The post does not contain functionality kn...                  0  \n",
      "2    No, the post does not contain functionality kn...                  0  \n",
      "3    No, the post does not contain functionality kn...                  0  \n",
      "4    No. \\n\\nThe provided post focuses on the use o...                  0  \n",
      "..                                                 ...                ...  \n",
      "835  No, the post does not contain alternative know...                  0  \n",
      "836  Yes, the post contains alternative knowledge a...                  1  \n",
      "837  No, the post does not contain alternative know...                  0  \n",
      "838  No, the post does not contain alternative know...                  0  \n",
      "839  Yes, the post contains alternative knowledge o...                  1  \n",
      "\n",
      "[840 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find some posts\n",
    "df = pd.read_csv(\"finetuned_retrieved_posts.csv\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "if {\"api\", \"knowledge_type\"}.issubset(df.columns):\n",
    "    second_to_fifth_rows = df\n",
    "    print(second_to_fifth_rows)\n",
    "else:\n",
    "    print(\"The columns 'api' and/or 'knowledge_type' do not exist in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences have been split and saved to 'post_sentence_split.csv'.\n"
     ]
    }
   ],
   "source": [
    "sentence_data = []\n",
    "\n",
    "for index, row in second_to_fifth_rows.iterrows():\n",
    "    api = row[\"api\"]\n",
    "    post = row[\"post\"]\n",
    "    post_id = row[\"post_id\"]\n",
    "    doc = nlp(post)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    for sentence in sentences:\n",
    "        sentence_data.append(\n",
    "            {\"api\": api, \"post\": post, \"post_id\": post_id, \"sentence\": sentence}\n",
    "        )\n",
    "\n",
    "sentence_df = pd.DataFrame(sentence_data)\n",
    "\n",
    "sentence_df.to_csv(\"post_sentence_split.csv\", index=False)\n",
    "print(\"Sentences have been split and saved to 'post_sentence_split.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_32244\\4026068888.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(sentence, \"html.parser\")\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_32244\\4026068888.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(sentence, \"html.parser\")\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_32244\\4026068888.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(sentence, \"html.parser\")\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_32244\\4026068888.py:71: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(sentence, \"html.parser\")\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_32244\\4026068888.py:78: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(sentence, \"html.parser\")\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_32244\\4026068888.py:83: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(sentence, \"html.parser\")\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_32244\\4026068888.py:98: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  creation_time = datetime.utcfromtimestamp(creation_date)\n",
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_32244\\4026068888.py:99: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.utcnow()\n"
     ]
    }
   ],
   "source": [
    "df = sentence_df\n",
    "df[\"num_tokens\"] = df[\"sentence\"].apply(count_tokens)\n",
    "df = is_codeblock(df)\n",
    "df[\"sentence_position\"] = df.groupby(\"post_id\").cumcount() + 1\n",
    "df[\"api_position\"] = df.apply(\n",
    "    lambda row: api_position(row[\"sentence\"], row[\"api\"]), axis=1\n",
    ")\n",
    "df[\"starts_lower_case\"] = df[\"sentence\"].apply(starts_lower_case)\n",
    "df[\"num_code_characters\"] = code_token_count(df)\n",
    "df[\"contains_html_tags\"] = df[\"sentence\"].apply(contains_html_tags)\n",
    "df[\"percentage_tagged_tokens\"] = df[\"sentence\"].apply(percentage_tagged_tokens)\n",
    "\n",
    "parent = pd.read_csv(\"parentpost.csv\")\n",
    "child = pd.read_csv(\"details_of_retrieved_posts.csv\")\n",
    "user = pd.read_csv(\"users.csv\")\n",
    "\n",
    "all_features = []\n",
    "for index, row in df.iterrows():\n",
    "    post_id = row[\"post_id\"]\n",
    "    answer_body = row[\"post\"]\n",
    "    api = row[\"api\"]\n",
    "    features = fetch_features(parent, child, user, post_id, answer_body, api)\n",
    "    all_features.append(features)\n",
    "\n",
    "for i in features:\n",
    "    temp = []\n",
    "    for j in all_features:\n",
    "        temp.append(j[i])\n",
    "    df[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16382\\AppData\\Local\\Temp\\ipykernel_32244\\1449982122.py:3: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(0)\n"
     ]
    }
   ],
   "source": [
    "df[\"sentence\"] = df[\"sentence\"].apply(remove_html_tags)\n",
    "\n",
    "df = df.fillna(0)\n",
    "for col in df.select_dtypes(include=[\"bool\"]).columns:\n",
    "    df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:weka.core.jvm:JVM already running, call jvm.stop() first\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "start()\n",
    "test_sentences = df[\"sentence\"].tolist()\n",
    "test_embeddings = generate_embedding(test_sentences)\n",
    "test_embeddings_df = pd.DataFrame(test_embeddings, index=df.index)\n",
    "df = pd.concat(\n",
    "    [df.reset_index(drop=True), test_embeddings_df.reset_index(drop=True)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"official_documents\"\n",
    "\n",
    "txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "\n",
    "official_document_dict = {}\n",
    "\n",
    "for file_path in txt_files:\n",
    "    api = file_path.split(\"\\\\\")[-1][:-4]\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        official_document_dict[api] = sent_tokenize(nlp, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes = []\n",
    "averages = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    api = row[\"api\"]\n",
    "    sentence = row[\"sentence\"]\n",
    "    sents = official_document_dict[api.lower()]\n",
    "    similarities = []\n",
    "    for s in sents:\n",
    "        similarities.append(token_similarity(s, sentence))\n",
    "    maxes.append(max(similarities))\n",
    "    averages.append(np.mean(similarities))\n",
    "\n",
    "df[\"max\"] = maxes\n",
    "df[\"mean\"] = averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_to_be_predicted_by_sise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model loaded successfully.\n",
      "scaler model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "with open(\"svm_model.pkl\", \"rb\") as f:\n",
    "    svm = pickle.load(f)\n",
    "print(\"SVM model loaded successfully.\")\n",
    "\n",
    "with open(\"scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "print(\"scaler model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_columns = [\"api\", \"post\", \"post_id\", \"sentence\"]\n",
    "feature_columns = [col for col in df.columns if col not in info_columns]\n",
    "\n",
    "X_test = df[feature_columns]\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "X_test = X_test[scaler.feature_names_in_]\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prediction\"] = y_pred\n",
    "\n",
    "predicted_ones = df[df[\"prediction\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences for API 'ActionBar' written to sise_output\\ActionBar.txt.\n",
      "Sentences for API 'BigDecimal' written to sise_output\\BigDecimal.txt.\n",
      "Sentences for API 'ByteArray' written to sise_output\\ByteArray.txt.\n",
      "Sentences for API 'GradientTape' written to sise_output\\GradientTape.txt.\n",
      "Sentences for API 'IllegalArgumentException' written to sise_output\\IllegalArgumentException.txt.\n",
      "Sentences for API 'MBeanServer' written to sise_output\\MBeanServer.txt.\n",
      "Sentences for API 'Manifest' written to sise_output\\Manifest.txt.\n",
      "Sentences for API 'MediaPlayer' written to sise_output\\MediaPlayer.txt.\n",
      "Sentences for API 'MessageDigest' written to sise_output\\MessageDigest.txt.\n",
      "Sentences for API 'Model' written to sise_output\\Model.txt.\n",
      "Sentences for API 'UUID' written to sise_output\\UUID.txt.\n",
      "Sentences for API 'VariableSynchronization' written to sise_output\\VariableSynchronization.txt.\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"sise_output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for api, group in predicted_ones.groupby(\"api\"):\n",
    "    file_path = os.path.join(output_folder, f\"{api}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(list(set(group[\"sentence\"]))))\n",
    "        print(f\"Sentences for API '{api}' written to {file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5986"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       If you're going to use the simplest JMS patter...\n",
       "31      If your design depends on what the observers n...\n",
       "35      The consumer's side is more likely to be changed.\n",
       "43      In other words, the broker must dispatch messa...\n",
       "54             I couldn't see your messageConverter bean.\n",
       "                              ...                        \n",
       "5950    ByteArray.\\nOn Kotlin/JVM, ByteArray compiles ...\n",
       "5966    /api/latest/jvm/stdlib/kotlin.text/to-byte-arr...\n",
       "5980    It works for an empty ByteArray, as well as on...\n",
       "5983    It can be used as follows:\\n@OptIn(Experimenta...\n",
       "5985    ByteArray = \\n    ByteArray (size) {i -&gt; (d...\n",
       "Name: sentence, Length: 596, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ones[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PostId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>CreationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78371072</td>\n",
       "      <td>2</td>\n",
       "      <td>76990190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9619685.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;You can configure your &lt;code&gt;RelyingPartyRe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-04-23 08:58:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77890382</td>\n",
       "      <td>2</td>\n",
       "      <td>56451209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12388097.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;To calculate gradients with respect to mult...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-27 07:26:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77780912</td>\n",
       "      <td>2</td>\n",
       "      <td>77738225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2087640.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Note that I'am not used to websphere nor to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-08 13:57:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77763064</td>\n",
       "      <td>2</td>\n",
       "      <td>77738225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8340997.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Update your configuration code to this :&lt;/p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-01-05 07:28:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77699058</td>\n",
       "      <td>2</td>\n",
       "      <td>77693148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436560.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;The reason for the criticism is that usuall...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-21 15:46:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PostId  PostTypeId  ParentId  AcceptedAnswerId  Score  ViewCount  \\\n",
       "0  78371072           2  76990190               NaN      2        NaN   \n",
       "1  77890382           2  56451209               NaN     -1        NaN   \n",
       "2  77780912           2  77738225               NaN      0        NaN   \n",
       "3  77763064           2  77738225               NaN      1        NaN   \n",
       "4  77699058           2  77693148               NaN      2        NaN   \n",
       "\n",
       "   OwnerUserId  Title                                               Body  \\\n",
       "0    9619685.0    NaN  <p>You can configure your <code>RelyingPartyRe...   \n",
       "1   12388097.0    NaN  <p>To calculate gradients with respect to mult...   \n",
       "2    2087640.0    NaN  <p>Note that I'am not used to websphere nor to...   \n",
       "3    8340997.0    NaN  <p>Update your configuration code to this :</p...   \n",
       "4     436560.0    NaN  <p>The reason for the criticism is that usuall...   \n",
       "\n",
       "   Tags         CreationDate  \n",
       "0   NaN  2024-04-23 08:58:55  \n",
       "1   NaN  2024-01-27 07:26:40  \n",
       "2   NaN  2024-01-08 13:57:07  \n",
       "3   NaN  2024-01-05 07:28:11  \n",
       "4   NaN  2023-12-21 15:46:43  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"details_of_retrieved_posts.csv\")\n",
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
