{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import replicate\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! set REPLICATE_API_TOKEN="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama2_70b_chat(prompt):\n",
    "    os.environ[\"REPLICATE_API_TOKEN\"] = \"\"\n",
    "    api = replicate.Client(api_token=os.environ[\"REPLICATE_API_TOKEN\"])\n",
    "    output = api.run(\n",
    "        \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "        input={\"prompt\": prompt},\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for item in output:\n",
    "        result += item\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def llama3_70b_instruct(prompt):\n",
    "    os.environ[\"REPLICATE_API_TOKEN\"] = \"\"\n",
    "    api = replicate.Client(api_token=os.environ[\"REPLICATE_API_TOKEN\"])\n",
    "    output = api.run(\"meta/meta-llama-3-70b-instruct\", input={\"prompt\": prompt})\n",
    "\n",
    "    result = \"\"\n",
    "    for item in output:\n",
    "        result += item\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def mistral_7b_instruct_v3(prompt):\n",
    "    openai = OpenAI(\n",
    "        api_key=\"\",\n",
    "        base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    )\n",
    "\n",
    "    chat_completion = openai.chat.completions.create(\n",
    "        model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4 = OpenAI_LLM(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_to_post = read_json(\"api_to_post\")\n",
    "benchmark = read_json(\"benchmark\")\n",
    "api_candidates = read_json(\"api_candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_extraction(\n",
    "    knowledge_type,\n",
    "    api,\n",
    "    api_description,\n",
    "    knowledge_type_description,\n",
    "    example_knowledge,\n",
    "    post,\n",
    "    temperature,\n",
    "):\n",
    "    with open(\"prompts/autodoc/knowledge_extraction/instruction.txt\", \"r\") as f:\n",
    "        user_prompt = f.read().format(\n",
    "            knowledge_type=knowledge_type,\n",
    "            api=api,\n",
    "            api_description=api_description,\n",
    "            knowledge_type_description=knowledge_type_description,\n",
    "            example_knowledge=example_knowledge,\n",
    "            post=post,\n",
    "        )\n",
    "    return gpt4.free_output(user_prompt, temperature=temperature)[0]\n",
    "\n",
    "\n",
    "def knowledge_validation(api, api_description, extracted_knowledge, post, temperature):\n",
    "    with open(\"prompts/autodoc/knowledge_validation/instruction.txt\", \"r\") as f:\n",
    "        user_prompt = f.read().format(\n",
    "            api=api,\n",
    "            api_description=api_description,\n",
    "            extracted_knowledge=extracted_knowledge,\n",
    "            post=post,\n",
    "        )\n",
    "    return gpt4.free_output(user_prompt, temperature=temperature)[0]\n",
    "\n",
    "\n",
    "def knowledge_summarization(api, knowledge_list, temperature):\n",
    "    with open(\"prompts/autodoc/knowledge_summarization/instruction.txt\", \"r\") as f:\n",
    "        user_prompt = f.read().format(api=api, knowledge_list=knowledge_list)\n",
    "    return gpt4.free_output(user_prompt, temperature=temperature)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_autodoc(api, model=\"gpt4o\", temperature=0.8, ablate=[]):\n",
    "    knowledge_type_description = {\n",
    "        \"functionality\": \"describes the actions or operations an API can perform\",\n",
    "        \"concept\": \"covers the foundational ideas and terminologies for understanding and effectively utilizing an API.\",\n",
    "        \"performance\": \"refers to the time and memory efficiency of an API.\",\n",
    "        \"directive\": \"is an essential type of knowledge that provides guidelines on the proper use of an API, including best practices to follow and actions to avoid.\",\n",
    "        \"pattern\": \"illustrates common use cases for applying the API to solve specific problems or achieve certain outcomes.\",\n",
    "        \"environment\": \"specifies the necessary conditions, system requirements, or configurations under which an API can function correctly.\",\n",
    "        \"alternative\": \"suggests other APIs offering similar functionality, which can be considered as replacements or complementary options.\",\n",
    "    }\n",
    "\n",
    "    knowledge_type_example = {\n",
    "        \"functionality\": \"tf.gather is used to select tensor elements at specific indices.\",\n",
    "        \"concept\": \"Tensor is essentially a high-dimensional array.\",\n",
    "        \"performance\": \"tf.gather has overhead when used on large tensors.\",\n",
    "        \"directive\": \"When using tf.gather, ensure indices are within the shape of the input tensor.\",\n",
    "        \"pattern\": \"tf.gather is commonly used in embedding lookup operations.\",\n",
    "        \"environment\": \"tf.gather requires TensorFlow installed and supports both CPU and GPU execution.\",\n",
    "        \"alternative\": \"Alternatives to tf.gather include tf.scatter_nd and tf.index_select.\",\n",
    "    }\n",
    "\n",
    "    for i in benchmark:\n",
    "        if i[\"name\"] == api:\n",
    "            api_description = i[\"description\"]\n",
    "            break\n",
    "    df = pd.read_csv(\"benchmark_csv/retrieved_post.csv\")\n",
    "    api_df = df[df[\"api\"] == api]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for index, row in api_df.iterrows():\n",
    "        result.append(\n",
    "            {\n",
    "                \"post_id\": row[\"post_id\"],\n",
    "                \"cleaned_post\": row[\"post\"],\n",
    "                \"knowledge_type\": row[\"knowledge_type\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    for i in result:\n",
    "        extraction_result = knowledge_extraction(\n",
    "            i[\"knowledge_type\"],\n",
    "            api,\n",
    "            api_description,\n",
    "            knowledge_type_description[i[\"knowledge_type\"]],\n",
    "            knowledge_type_example[i[\"knowledge_type\"]],\n",
    "            i[\"cleaned_post\"],\n",
    "            temperature,\n",
    "        )\n",
    "        i[\"extraction_result\"] = extraction_result\n",
    "\n",
    "    for i in result:\n",
    "        if i[\"extraction_result\"] != \"No such knowledge\":\n",
    "            validation_result = knowledge_validation(\n",
    "                api, api_description, extraction_result, i[\"cleaned_post\"], temperature\n",
    "            )\n",
    "            i[\"validation_result\"] = validation_result\n",
    "\n",
    "    knowledge_list = \"\"\n",
    "    for i in result:\n",
    "        if \"validation_result\" in i and i[\"validation_result\"] == \"Yes\":\n",
    "            knowledge_list += (\n",
    "                f\"{i['extraction_result']}. Knowledge type: {i['knowledge_type']}\\n\"\n",
    "            )\n",
    "    summarization_result = knowledge_summarization(api, knowledge_list, temperature)\n",
    "    return summarization_result, knowledge_list, \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
